# !pip install transformers==4.43.2
!pip install bitsandbytes==0.43.3
!pip install -U bitsandbytes
!pip install accelerate==0.33.0
import pandas as pd
import warnings
warnings.filterwarnings("ignore", message="Setting `pad_token_id` to `eos_token_id`")

from google.colab import drive
drive.mount('/content/drive')
### 모델 불러오기
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import login

# HF_TOKEN = os.getenv("HF_TOKEN")
login(token="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")
def load_model(base_model, torch_dtype=torch.bfloat16, use_quantization=False, quantization_config=None):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    attn_implementation = "eager"

    if use_quantization:
        if quantization_config is None:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch_dtype,
                bnb_4bit_use_double_quant=True,
            )
    else:
        bnb_config = None

    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        quantization_config=bnb_config,
        torch_dtype=torch_dtype,
        device_map="auto",
        attn_implementation=attn_implementation
    )

    if not use_quantization:
        model.to(device)

    return model
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

torch_dtype = torch.float16
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
model = load_model(model_name, use_quantization=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)
config = model.config


print(f"Memory Allocated: {round(torch.cuda.memory_allocated() / (1024 ** 2), 3)}GB")
print(f"Memory Reserved: {round(torch.cuda.memory_reserved() / (1024 ** 2), 3)}GB")
### 프롬프트 작성하기
example = """
리뷰='''
맛 별로 쟁여템이에요
비상식량으로 비축해 둡니다
'''
positive

리뷰='''
고추장 찌개 평범해요
그냥 먹을 만은합니다
'''
neutral

리뷰='''
그냥저냥 보통이였어요
'''
neutral

리뷰='''
이거는 도대체 무슨 맛인지 모르겠네요
'''
negative

리뷰='''
갈비는 아주 아주 작은 거 세 개에 갈비탕 맛은 없고
안 매운 고추만을 넣어 삶은 물을 마시는 느낌'
'''
negative

리뷰='''
물 반 컵 정도 더 넣었는데도 짜요
'''
negative

리뷰='''
건더기는 괜찮았는데 국물이 별로네요 크림 맛 강함
'''
negative

리뷰='''
헉 너무 맛있게 먹었는데 국 속에서 이런 게 나왔어요 이거 아무리 봐도 무슨 애벌레 같은 느낌인데  으아 이게 무슨 일이야 맛있긴했는데 이 기분 어쩔
'''
negative

리뷰='''
무국은 파는 곳이 없어서 가끔 너무 먹고 싶은데 무를 사서 요리하면 너무 많아서 급히 주문해서 아침에 받자마자 끓여 먹었어요 역시 무는 냉동했다 먹으면 식감이 영 별로에요 그래도 많이 짜지 않고 시중 레토르트 식품보다는 훨씬 맛있어요 제 입엔 조금 짜서 물 섞어 먹었어 오
'''
positive

리뷰='''
파 듬뿍 넣어서 먹었는데 시원하고 너무 맛있더라구요 2 인 나눠서 국으로 먹었는데 양 괜찮았어요
'''
positive

리뷰='''
생각보다 닭이 실하고
국물이 진한 편이예요
백숙 맛이 진하고 좋아요
'''
positive

리뷰='''
감동적인 맛 재구매 다른 김치 우동류보다 젤
맛있음 다만 1인분씩 양념 면 오뎅이 분할되어 있음 좋겠음
'''
positive


리뷰='''
물을 240ml 더 넣고 끓여서 먹었어요
맛은 너무 좋은데 짜서 물 더 부으니
라면 사리도 넣고 맛있게 먹었습니다
햄은 종류별로 많이 들어 있었어요
''''
positive
"""
prompt = f"""
당신은 상품에 대한 고객의 리뷰를 감정에 따라 분류하는 소비자 분석가이다.

**지시사항:**
주어진 리뷰에 드러나는 가장 주된 감정을 분석하여
'positive', 'negative', 'neutral' 중 하나의 감정으로 태깅하라.
리뷰의 문맥을 충분히 반영해야하며, 각 리뷰는 한 감정에만 태깅된다.

**출력 방식**
'positive' 혹은 'negative' 혹은 'neutral' 중 하나만 출력한다.
다른 어떠한 추가 설명도 함께 출력해서는 안된다.

**예시**
{example}
"""
# def extract_assistant_text(outputs) :
#     decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

#     start_idx = decoded_output.find("assistant")
#     if start_idx != -1:
#         assistant_reply = decoded_output[start_idx + len("assistant"):].strip().strip("'")
#     return assistant_reply

def extract_assistant_text(outputs):
    return tokenizer.decode(outputs[0], skip_special_tokens=True).split("assistant")[-1].strip()
query = '''
양 곰탕 맛 집에서 먹는 것보다 맛있어서 몇 개 더 쟁였어요

조리법
포장과 조리법이 잘 되어 있어서 먹기에도 정말 편한데요
한 반 양 곰탕은 찬물에 20분 정도 넣었다가 끓여 먹으면 돼요
저는 전자레인지에도 돌려 봤는데 맛이 똑같더라구요

꿀 팁
이미 양으로도 완벽하지만 더 맛있게 드시려면
대파나 쪽파 올린 다음 후추 많이 뿌려서 드시면 더 좋아요
확실히 파가 있고 없고가 크게 차이 나니까
집에 파 있는 분들은 아낌없이 넣으세요
'''
inputs = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''
outputs = model.generate(**tokenizer(inputs, return_tensors="pt").to(device), max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)

extract_assistant_text(outputs)
### 데이터 적용하기
origin = pd.read_csv("./drive/MyDrive/deep daiv/50개씩.csv")
origin['product_id'].value_counts()[-15:]
# origin[origin['product_id']==1000239338]['cleaned_Kiwi_review']
data=[]
for text in origin[origin['product_id']==1000239338]['cleaned_Kiwi_review']:
    query=text
    inputs = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''
    outputs = model.generate(**tokenizer(inputs, return_tensors="pt").to(device), max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)
    data.append([query, extract_assistant_text(outputs)])
df = pd.DataFrame(data, columns=['review', 'sentiment'])
df
df['sentiment'].value_counts()
# df['sentiment']=df['sentiment'].apply(lambda x: x.strip("'"))
for i in range(df.shape[0]):
    if df.loc[i,'sentiment'] == "negative":
        print(i, df.loc[i,:])
query = origin[origin['product_id']==1000408458].iloc[86,:]['cleaned_Kiwi_review']
inputs = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''
outputs = model.generate(**tokenizer(inputs, return_tensors="pt").to(device), max_new_tokens=1500, pad_token_id=tokenizer.eos_token_id)
# re = tokenizer.decode(outputs[0], skip_special_tokens=True).split("assistant")[-1]#.split('\n')[-1].strip()
# re
df.loc[14, 'sentiment'] = 'neutral'
df.iloc[14, :]
part = origin[origin['product_id']==1000239338][['product_id', 'product_type', 'date', 'help_num', 'cleaned_Kiwi_review']]
part = pd.concat([part.reset_index(drop=True), df['sentiment'].reset_index(drop=True)], axis=1)
part
part.to_csv("./drive/MyDrive/deep daiv/감정 태깅 결과/1000331934_감정태깅.csv", index=False, encoding="utf-8-sig")




안된거 = [5026468, 5042765, 1000040783, 5056783, 5133142, 5061723, 5044756, 5097307, 5001235, 5148360,
       1000085968, 5119471, 5097092, 5114061, 5030967, 5144535, 5134147, 5030966, 5047801, 5066024,
       5095132, 5103533, 5119423, 5044518, 5062297, 1000047947, 5100476, 1000047949, 5114696, 1000023238, 5082999, 1000052923, 1000106177, 5068037, 5049682, 1000315306, 1000370851, 1000272180, 1000015143, 5115296, 5100778, 5156992, 1000331934, 1000408458, 1000659319, 1000431402, 1000408456, 1000498621, 1000498619, 1000356831]
안된거[17:20]
for i in 안된거[17:20]:
  data=[]
  for text in origin[origin['product_id']==i]['cleaned_Kiwi_review']:
      query=text
      inputs = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''
      outputs = model.generate(**tokenizer(inputs, return_tensors="pt").to(device), max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)
      data.append([query, extract_assistant_text(outputs)])

  df = pd.DataFrame(data, columns=['review', 'sentiment'])

  part = origin[origin['product_id']==i][['product_id', 'product_type', 'date', 'help_num', 'cleaned_Kiwi_review']]
  part = pd.concat([part.reset_index(drop=True), df['sentiment'].reset_index(drop=True)], axis=1)

  part.to_csv(f"./drive/MyDrive/deep daiv/추가 태깅/{i}_감정태깅.csv", index=False, encoding="utf-8-sig")




# print(extract_assistant_text(tokenizer.decode(outputs[0]).split(inputs), inputs))
#print(tokenizer.decode(outputs[0]).split(inputs)[1])
